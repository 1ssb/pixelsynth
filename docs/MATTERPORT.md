### Matterport3D

#### Download
*Download steps closely adapted from [SynSin](https://github.com/facebookresearch/synsin/blob/master/MP3D.md).*

1. Download habitat:
- [habitat-api](https://github.com/facebookresearch/habitat-api) (if you want to use Matterport datasets)
- [habitat-sim](https://github.com/facebookresearch/habitat-sim) (if you want to use Matterport datasets)

2. Download the [point nav datasets](https://github.com/facebookresearch/habitat-api#task-datasets).

3. Download [MP3D](https://niessner.github.io/Matterport/). 
We thank Angela Dai of the Matterport team for allowing us to share our test frames for interested parties. 
To download our test frames, add a flag to the download script for Matterport3D. The command is:
`python download_mp.py --task_data pixelsynth -o [output_dir]` (using python 2.7).

#### Update options

Update the paths in `./options/options.py` for the dataset being used.

#### Training & Evaluation Note

Some variables containing data in shell scripts have been set to my local directory `/x/cnris/...` as an example. CUDA_VISIBLE_DEVICES is also used to show required GPUs of each script. Change both as needed.

### Training

Training is split into 3 components: 

1. Train VQ-VAE
- `sh scripts/extract_vqvae_dset_mp3d.sh`: select subset of images from Matterport3D to use for training (32k) & validation (8k).
- `sh scripts/train_vqvae_mp3d.sh`: train for 150 epochs with batch size 120
- `sh scripts/extract_code_mp3d.sh`: with trained vqvae, extract codes for 40k set

2. Train the depth, projection, and refinement module (using frozen VQ-VAE)
- `sh scripts/train_dpr_mp3d.sh`: train for 250 epochs with batch size 12 on full Matterport3D dataset
- `sh scripts/extract_pixcnn_orders_mp3d.sh`: with trained depth model, extract orderings used for outpainting on 40k set

3. Train Custom-Order PixelCNN++ (using VQ-VAE embeddings and orderings from depth model)
- `sh scripts/train_lmconv_mp3d.sh`: train for 150 epochs with batch size 120

### Evaluation

TIP: Autoregressive sampling is slow - especially if using many samples! Below evaluation code is run on a single GPU; we recommend running multiple times across GPUs with different splits of the evaluation set for faster inference.

#### Pretrained Model and SynSin Baselines
Our pretrained model is available for download [here](https://fouheylab.eecs.umich.edu/~cnris/pixelsynth/modelcheckpoints/mp3d/pixelsynth.pth). SynSin - 6X (Our main baseline - SynSin trained on rotations consistent with our model) is available [here](https://fouheylab.eecs.umich.edu/~cnris/pixelsynth/modelcheckpoints/mp3d/synsin_6x.pth).
SynSin and several other baselines are available for download at its [Github](https://github.com/facebookresearch/synsin). Place these models in new directory modelcheckpoints/mp3d.

#### Evaluating Quality

Evaluating quality evaluates single predicted output given an input image and camera transform. Evaluating consists of two steps:

1. `sh scripts/eval_quality_mp3d.sh`: predict output for all images.
2. `python calc_errors_quality.py`: compare outputs to ground truth using FID, Perc Sim and PSNR. Adjust the variables `names`, `base`, `imagedir`, `copy`, and `sampled` as needed

Updated versions of Pytorch, seeding, etc. mean results will closely match paper but not exactly.
Results generated by our model precisely replicating paper results are available [here](https://fouheylab.eecs.umich.edu/~cnris/pixelsynth/our_results/quality/mp3d/ours.zip).

#### Evaluating Consistency

Evaluating consistency evaluates the consistency of two predicted outputs given an input image and camera transform. 
The first output is this full rotation and translation, the second is halfway between this and the input. 

On Matterport, we evaluate consistency using camera transformations involving both rotation and translation via `sh scripts/eval_consistency_mp3d.sh`. There is no clear automated metric to evaluate, so we rely on A/B testing to compare consistency across models.

Updated versions of Pytorch, seeding, etc. mean results will closely match paper but not exactly.
Results generated by our model precisely replicating paper results are available [here](https://fouheylab.eecs.umich.edu/~cnris/pixelsynth/our_results/consistency/mp3d/ours.zip).
